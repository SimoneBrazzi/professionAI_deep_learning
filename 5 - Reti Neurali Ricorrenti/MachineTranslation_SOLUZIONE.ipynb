{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc33373",
   "metadata": {},
   "source": [
    "## Esercitazione: Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f6a32",
   "metadata": {},
   "source": [
    "Costruire un modello sequenziale per la traduzione (dall'inglese all'italiano). Il modello prende una sequenza in inglese e torna in output una sequenza in italiano:\n",
    "1. Costruire le sequenze ed effettuare il padding per entrambe le lingue (NB: le sequenze vanno paddate alla maxlen di entrambe le lingue)\n",
    "2. Dividi il dataset tra train e test con il 20% di test_size\n",
    "2. Definire un modello che abbia uno strato di embedding e almeno due strati ricorrenti e in uscita uno strato Dense con il numero di neuroni pari al vocabolario per la traduzione (italiano)\n",
    "3. NB: Per migliorare le performance sullo strato Dense conviene applicare un layer TimeDistributed in questo modo\n",
    "        TimeDistributed(Dense())\n",
    "3. Eseguire l'addestramento per almeno 100 epoche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1295ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"machine_translation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c9b9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>italian</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tom portò i suoi.</td>\n",
       "      <td>tom brought his.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a te non piace il pesce?</td>\n",
       "      <td>don't you like fish?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non abbiamo mai riso.</td>\n",
       "      <td>we never laughed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aspetti un momento.</td>\n",
       "      <td>hang on a moment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quando è finito?</td>\n",
       "      <td>when did that end?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    italian               english\n",
       "0         tom portò i suoi.      tom brought his.\n",
       "1  a te non piace il pesce?  don't you like fish?\n",
       "2     non abbiamo mai riso.     we never laughed.\n",
       "3       aspetti un momento.     hang on a moment.\n",
       "4          quando è finito?    when did that end?"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "500ca121",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = df.english.values\n",
    "italian_sentences = df.italian.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c790ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(english_sentences,italian_sentences, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fce3739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 08:42:15.260121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "eng_tokenizer = Tokenizer()\n",
    "ita_tokenizer = Tokenizer()\n",
    "\n",
    "eng_tokenizer.fit_on_texts(X_train)\n",
    "ita_tokenizer.fit_on_texts(Y_train)\n",
    "\n",
    "\n",
    "#sequences\n",
    "eng_sequences_train = eng_tokenizer.texts_to_sequences(X_train)\n",
    "ita_sequences_train = ita_tokenizer.texts_to_sequences(Y_train)\n",
    "eng_sequences_test = eng_tokenizer.texts_to_sequences(X_test)\n",
    "ita_sequences_test = ita_tokenizer.texts_to_sequences(Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ff3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_ita = len(max(ita_sequences_train,key=len))\n",
    "maxlen_eng = len(max(eng_sequences_train,key=len))\n",
    "\n",
    "\n",
    "#train padding\n",
    "padded_eng_sentences_train = pad_sequences(eng_sequences_train, padding = 'pre', maxlen = maxlen_eng)\n",
    "padded_ita_sentences_train = pad_sequences(ita_sequences_train, padding = 'pre', maxlen = maxlen_ita)\n",
    "\n",
    "\n",
    "#test padding\n",
    "padded_eng_sentences_test = pad_sequences(eng_sequences_test, padding = 'pre', maxlen = maxlen_eng)\n",
    "padded_ita_sentences_test = pad_sequences(ita_sequences_test, padding = 'pre', maxlen = maxlen_ita)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74be1ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Italian sentence length: 10\n",
      "Max English sentence length: 6\n",
      "Italian vocabulary size: 10005\n",
      "English vocabulary size: 4932\n"
     ]
    }
   ],
   "source": [
    "italian_vocab_size = len(ita_tokenizer.word_index)+1\n",
    "english_vocab_size = len(eng_tokenizer.word_index)+1\n",
    "print(\"Max Italian sentence length: {}\".format(padded_ita_sentences_train.shape[1]))\n",
    "print(\"Max English sentence length: {}\".format(padded_eng_sentences_train.shape[1]))\n",
    "print(\"Italian vocabulary size: {}\".format(italian_vocab_size))\n",
    "print(\"English vocabulary size: {}\".format(english_vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5505e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x = pad_sequences(padded_eng_sentences_train, maxlen_ita)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5898d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8336ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 08:42:32.324370: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 128)           631296    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 10, 128)          98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 10, 128)          98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 10, 10005)        1290645   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,119,573\n",
      "Trainable params: 2,119,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.backend import clear_session\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, LSTM, TimeDistributed, Dense, Bidirectional\n",
    "\n",
    "clear_session()\n",
    "model = Sequential()\n",
    "model.add(Embedding(english_vocab_size, 128, input_length=maxlen_ita))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, activation=\"tanh\")))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, activation=\"tanh\")))\n",
    "model.add(TimeDistributed(Dense(italian_vocab_size, activation=\"softmax\")))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8011e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680b7ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "63/63 [==============================] - 61s 898ms/step - loss: 3.7523 - accuracy: 0.6417 - val_loss: 2.4695 - val_accuracy: 0.6536\n",
      "Epoch 2/180\n",
      "63/63 [==============================] - 57s 901ms/step - loss: 2.3789 - accuracy: 0.6549 - val_loss: 2.3395 - val_accuracy: 0.6615\n",
      "Epoch 3/180\n",
      "63/63 [==============================] - 57s 908ms/step - loss: 2.2864 - accuracy: 0.6636 - val_loss: 2.2903 - val_accuracy: 0.6664\n",
      "Epoch 4/180\n",
      "63/63 [==============================] - 57s 900ms/step - loss: 2.2238 - accuracy: 0.6678 - val_loss: 2.2517 - val_accuracy: 0.6694\n",
      "Epoch 5/180\n",
      "63/63 [==============================] - 57s 911ms/step - loss: 2.1753 - accuracy: 0.6699 - val_loss: 2.2133 - val_accuracy: 0.6728\n",
      "Epoch 6/180\n",
      "63/63 [==============================] - 56s 897ms/step - loss: 2.1320 - accuracy: 0.6726 - val_loss: 2.1736 - val_accuracy: 0.6754\n",
      "Epoch 7/180\n",
      "63/63 [==============================] - 57s 901ms/step - loss: 2.0929 - accuracy: 0.6759 - val_loss: 2.1349 - val_accuracy: 0.6787\n",
      "Epoch 8/180\n",
      "63/63 [==============================] - 56s 897ms/step - loss: 2.0456 - accuracy: 0.6796 - val_loss: 2.1061 - val_accuracy: 0.6810\n",
      "Epoch 9/180\n",
      "63/63 [==============================] - 57s 912ms/step - loss: 1.9950 - accuracy: 0.6837 - val_loss: 2.0447 - val_accuracy: 0.6884\n",
      "Epoch 10/180\n",
      "63/63 [==============================] - 56s 894ms/step - loss: 1.9497 - accuracy: 0.6893 - val_loss: 2.0070 - val_accuracy: 0.6923\n",
      "Epoch 11/180\n",
      "63/63 [==============================] - 57s 905ms/step - loss: 1.9096 - accuracy: 0.6937 - val_loss: 1.9731 - val_accuracy: 0.6951\n",
      "Epoch 12/180\n",
      "63/63 [==============================] - 57s 905ms/step - loss: 1.8704 - accuracy: 0.6969 - val_loss: 1.9390 - val_accuracy: 0.6978\n",
      "Epoch 13/180\n",
      "63/63 [==============================] - 56s 896ms/step - loss: 1.8321 - accuracy: 0.6998 - val_loss: 1.9163 - val_accuracy: 0.6992\n",
      "Epoch 14/180\n",
      "63/63 [==============================] - 57s 906ms/step - loss: 1.7922 - accuracy: 0.7035 - val_loss: 1.8694 - val_accuracy: 0.7039\n",
      "Epoch 15/180\n",
      "63/63 [==============================] - 56s 895ms/step - loss: 1.7531 - accuracy: 0.7070 - val_loss: 1.8410 - val_accuracy: 0.7062\n",
      "Epoch 16/180\n",
      "63/63 [==============================] - 56s 894ms/step - loss: 1.7156 - accuracy: 0.7105 - val_loss: 1.8089 - val_accuracy: 0.7093\n",
      "Epoch 17/180\n",
      "63/63 [==============================] - 56s 896ms/step - loss: 1.6788 - accuracy: 0.7139 - val_loss: 1.7776 - val_accuracy: 0.7127\n",
      "Epoch 18/180\n",
      "63/63 [==============================] - 56s 895ms/step - loss: 1.6426 - accuracy: 0.7174 - val_loss: 1.7524 - val_accuracy: 0.7157\n",
      "Epoch 19/180\n",
      "63/63 [==============================] - 56s 887ms/step - loss: 1.6080 - accuracy: 0.7208 - val_loss: 1.7163 - val_accuracy: 0.7189\n",
      "Epoch 20/180\n",
      "63/63 [==============================] - 55s 868ms/step - loss: 1.5727 - accuracy: 0.7244 - val_loss: 1.7007 - val_accuracy: 0.7195\n",
      "Epoch 21/180\n",
      "63/63 [==============================] - 57s 911ms/step - loss: 1.5385 - accuracy: 0.7277 - val_loss: 1.6659 - val_accuracy: 0.7240\n",
      "Epoch 22/180\n",
      "63/63 [==============================] - 56s 892ms/step - loss: 1.5051 - accuracy: 0.7311 - val_loss: 1.6357 - val_accuracy: 0.7276\n",
      "Epoch 23/180\n",
      "63/63 [==============================] - 56s 884ms/step - loss: 1.4733 - accuracy: 0.7348 - val_loss: 1.6077 - val_accuracy: 0.7304\n",
      "Epoch 24/180\n",
      "63/63 [==============================] - 56s 894ms/step - loss: 1.4415 - accuracy: 0.7388 - val_loss: 1.5927 - val_accuracy: 0.7320\n",
      "Epoch 25/180\n",
      "63/63 [==============================] - 57s 899ms/step - loss: 1.4108 - accuracy: 0.7426 - val_loss: 1.5590 - val_accuracy: 0.7361\n",
      "Epoch 26/180\n",
      "63/63 [==============================] - 56s 887ms/step - loss: 1.3806 - accuracy: 0.7459 - val_loss: 1.5371 - val_accuracy: 0.7379\n",
      "Epoch 27/180\n",
      "63/63 [==============================] - 56s 893ms/step - loss: 1.3504 - accuracy: 0.7498 - val_loss: 1.5132 - val_accuracy: 0.7400\n",
      "Epoch 28/180\n",
      "63/63 [==============================] - 56s 897ms/step - loss: 1.3223 - accuracy: 0.7531 - val_loss: 1.4961 - val_accuracy: 0.7426\n",
      "Epoch 29/180\n",
      "63/63 [==============================] - 57s 903ms/step - loss: 1.2944 - accuracy: 0.7563 - val_loss: 1.4719 - val_accuracy: 0.7466\n",
      "Epoch 30/180\n",
      "63/63 [==============================] - 59s 937ms/step - loss: 1.2670 - accuracy: 0.7599 - val_loss: 1.4502 - val_accuracy: 0.7484\n",
      "Epoch 31/180\n",
      "63/63 [==============================] - 71s 1s/step - loss: 1.2402 - accuracy: 0.7633 - val_loss: 1.4348 - val_accuracy: 0.7519\n",
      "Epoch 32/180\n",
      "63/63 [==============================] - 71s 1s/step - loss: 1.2141 - accuracy: 0.7670 - val_loss: 1.4107 - val_accuracy: 0.7539\n",
      "Epoch 33/180\n",
      "63/63 [==============================] - 73s 1s/step - loss: 1.1894 - accuracy: 0.7699 - val_loss: 1.3920 - val_accuracy: 0.7559\n",
      "Epoch 34/180\n",
      "63/63 [==============================] - 70s 1s/step - loss: 1.1642 - accuracy: 0.7734 - val_loss: 1.3778 - val_accuracy: 0.7592\n",
      "Epoch 35/180\n",
      "63/63 [==============================] - 63s 1s/step - loss: 1.1407 - accuracy: 0.7762 - val_loss: 1.3657 - val_accuracy: 0.7605\n",
      "Epoch 36/180\n",
      "63/63 [==============================] - 58s 920ms/step - loss: 1.1168 - accuracy: 0.7795 - val_loss: 1.3418 - val_accuracy: 0.7637\n",
      "Epoch 37/180\n",
      "63/63 [==============================] - 56s 893ms/step - loss: 1.0927 - accuracy: 0.7828 - val_loss: 1.3277 - val_accuracy: 0.7649\n",
      "Epoch 38/180\n",
      "63/63 [==============================] - 57s 914ms/step - loss: 1.0702 - accuracy: 0.7855 - val_loss: 1.3145 - val_accuracy: 0.7659\n",
      "Epoch 39/180\n",
      "63/63 [==============================] - 57s 904ms/step - loss: 1.0489 - accuracy: 0.7882 - val_loss: 1.2984 - val_accuracy: 0.7693\n",
      "Epoch 40/180\n",
      "63/63 [==============================] - 57s 900ms/step - loss: 1.0271 - accuracy: 0.7911 - val_loss: 1.2862 - val_accuracy: 0.7706\n",
      "Epoch 41/180\n",
      "63/63 [==============================] - 57s 899ms/step - loss: 1.0065 - accuracy: 0.7938 - val_loss: 1.2743 - val_accuracy: 0.7714\n",
      "Epoch 42/180\n",
      "63/63 [==============================] - 57s 903ms/step - loss: 0.9867 - accuracy: 0.7963 - val_loss: 1.2578 - val_accuracy: 0.7749\n",
      "Epoch 43/180\n",
      "63/63 [==============================] - 57s 899ms/step - loss: 0.9671 - accuracy: 0.7990 - val_loss: 1.2476 - val_accuracy: 0.7736\n",
      "Epoch 44/180\n",
      "63/63 [==============================] - 57s 900ms/step - loss: 0.9483 - accuracy: 0.8016 - val_loss: 1.2354 - val_accuracy: 0.7750\n",
      "Epoch 45/180\n",
      "63/63 [==============================] - 56s 897ms/step - loss: 0.9291 - accuracy: 0.8041 - val_loss: 1.2203 - val_accuracy: 0.7777\n",
      "Epoch 46/180\n",
      "63/63 [==============================] - 56s 889ms/step - loss: 0.9107 - accuracy: 0.8066 - val_loss: 1.2126 - val_accuracy: 0.7783\n",
      "Epoch 47/180\n",
      "63/63 [==============================] - 56s 894ms/step - loss: 0.8935 - accuracy: 0.8089 - val_loss: 1.2033 - val_accuracy: 0.7790\n",
      "Epoch 48/180\n",
      "63/63 [==============================] - 57s 899ms/step - loss: 0.8768 - accuracy: 0.8109 - val_loss: 1.1912 - val_accuracy: 0.7816\n",
      "Epoch 49/180\n",
      "63/63 [==============================] - 56s 893ms/step - loss: 0.8616 - accuracy: 0.8128 - val_loss: 1.1841 - val_accuracy: 0.7815\n",
      "Epoch 50/180\n",
      "63/63 [==============================] - 58s 920ms/step - loss: 0.8453 - accuracy: 0.8152 - val_loss: 1.1749 - val_accuracy: 0.7837\n",
      "Epoch 51/180\n",
      "63/63 [==============================] - 56s 892ms/step - loss: 0.8300 - accuracy: 0.8171 - val_loss: 1.1673 - val_accuracy: 0.7850\n",
      "Epoch 52/180\n",
      "63/63 [==============================] - 57s 903ms/step - loss: 0.8153 - accuracy: 0.8198 - val_loss: 1.1628 - val_accuracy: 0.7824\n",
      "Epoch 53/180\n",
      "63/63 [==============================] - 57s 912ms/step - loss: 0.7998 - accuracy: 0.8218 - val_loss: 1.1473 - val_accuracy: 0.7864\n",
      "Epoch 54/180\n",
      "63/63 [==============================] - 57s 908ms/step - loss: 0.7854 - accuracy: 0.8235 - val_loss: 1.1403 - val_accuracy: 0.7886\n",
      "Epoch 55/180\n",
      "63/63 [==============================] - 58s 916ms/step - loss: 0.7710 - accuracy: 0.8259 - val_loss: 1.1384 - val_accuracy: 0.7865\n",
      "Epoch 56/180\n",
      "63/63 [==============================] - 57s 908ms/step - loss: 0.7566 - accuracy: 0.8280 - val_loss: 1.1231 - val_accuracy: 0.7885\n",
      "Epoch 57/180\n",
      "63/63 [==============================] - 57s 910ms/step - loss: 0.7440 - accuracy: 0.8300 - val_loss: 1.1235 - val_accuracy: 0.7907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/180\n",
      "63/63 [==============================] - 56s 885ms/step - loss: 0.7311 - accuracy: 0.8317 - val_loss: 1.1104 - val_accuracy: 0.7922\n",
      "Epoch 59/180\n",
      "63/63 [==============================] - 57s 908ms/step - loss: 0.7184 - accuracy: 0.8337 - val_loss: 1.1060 - val_accuracy: 0.7892\n",
      "Epoch 60/180\n",
      "63/63 [==============================] - 57s 904ms/step - loss: 0.7066 - accuracy: 0.8351 - val_loss: 1.1014 - val_accuracy: 0.7935\n",
      "Epoch 61/180\n",
      "63/63 [==============================] - 56s 896ms/step - loss: 0.6950 - accuracy: 0.8370 - val_loss: 1.0938 - val_accuracy: 0.7927\n",
      "Epoch 62/180\n",
      "63/63 [==============================] - 57s 906ms/step - loss: 0.6834 - accuracy: 0.8386 - val_loss: 1.0832 - val_accuracy: 0.7937\n",
      "Epoch 63/180\n",
      "63/63 [==============================] - 57s 904ms/step - loss: 0.6719 - accuracy: 0.8406 - val_loss: 1.0804 - val_accuracy: 0.7952\n",
      "Epoch 64/180\n",
      "63/63 [==============================] - 57s 906ms/step - loss: 0.6602 - accuracy: 0.8422 - val_loss: 1.0877 - val_accuracy: 0.7949\n",
      "Epoch 65/180\n",
      "63/63 [==============================] - 57s 909ms/step - loss: 0.6500 - accuracy: 0.8442 - val_loss: 1.0714 - val_accuracy: 0.7936\n",
      "Epoch 66/180\n",
      "63/63 [==============================] - 57s 910ms/step - loss: 0.6393 - accuracy: 0.8455 - val_loss: 1.0639 - val_accuracy: 0.7954\n",
      "Epoch 67/180\n",
      "63/63 [==============================] - 56s 889ms/step - loss: 0.6293 - accuracy: 0.8469 - val_loss: 1.0621 - val_accuracy: 0.7967\n",
      "Epoch 68/180\n",
      "63/63 [==============================] - 57s 907ms/step - loss: 0.6200 - accuracy: 0.8481 - val_loss: 1.0509 - val_accuracy: 0.7969\n",
      "Epoch 69/180\n",
      "63/63 [==============================] - 57s 904ms/step - loss: 0.6094 - accuracy: 0.8496 - val_loss: 1.0467 - val_accuracy: 0.7979\n",
      "Epoch 70/180\n",
      "63/63 [==============================] - 57s 910ms/step - loss: 0.6005 - accuracy: 0.8514 - val_loss: 1.0461 - val_accuracy: 0.7980\n",
      "Epoch 71/180\n",
      "63/63 [==============================] - 57s 906ms/step - loss: 0.5908 - accuracy: 0.8530 - val_loss: 1.0417 - val_accuracy: 0.7990\n",
      "Epoch 72/180\n",
      "63/63 [==============================] - 58s 916ms/step - loss: 0.5816 - accuracy: 0.8542 - val_loss: 1.0345 - val_accuracy: 0.7971\n",
      "Epoch 73/180\n",
      "63/63 [==============================] - 56s 888ms/step - loss: 0.5724 - accuracy: 0.8557 - val_loss: 1.0403 - val_accuracy: 0.7993\n",
      "Epoch 74/180\n",
      "63/63 [==============================] - 57s 903ms/step - loss: 0.5638 - accuracy: 0.8568 - val_loss: 1.0311 - val_accuracy: 0.7982\n",
      "Epoch 75/180\n",
      "63/63 [==============================] - 57s 905ms/step - loss: 0.5550 - accuracy: 0.8585 - val_loss: 1.0263 - val_accuracy: 0.7995\n",
      "Epoch 76/180\n",
      "63/63 [==============================] - 55s 879ms/step - loss: 0.5471 - accuracy: 0.8592 - val_loss: 1.0200 - val_accuracy: 0.7984\n",
      "Epoch 77/180\n",
      "63/63 [==============================] - 56s 896ms/step - loss: 0.5387 - accuracy: 0.8605 - val_loss: 1.0270 - val_accuracy: 0.8011\n",
      "Epoch 78/180\n",
      "63/63 [==============================] - 56s 889ms/step - loss: 0.5312 - accuracy: 0.8617 - val_loss: 1.0170 - val_accuracy: 0.8004\n",
      "Epoch 79/180\n",
      "63/63 [==============================] - 57s 909ms/step - loss: 0.5235 - accuracy: 0.8631 - val_loss: 1.0148 - val_accuracy: 0.8020\n",
      "Epoch 80/180\n",
      "63/63 [==============================] - 58s 927ms/step - loss: 0.5156 - accuracy: 0.8646 - val_loss: 1.0127 - val_accuracy: 0.7991\n",
      "Epoch 81/180\n",
      "63/63 [==============================] - 59s 935ms/step - loss: 0.5083 - accuracy: 0.8655 - val_loss: 1.0088 - val_accuracy: 0.8028\n",
      "Epoch 82/180\n",
      "63/63 [==============================] - 56s 896ms/step - loss: 0.5006 - accuracy: 0.8670 - val_loss: 1.0075 - val_accuracy: 0.7999\n",
      "Epoch 83/180\n",
      "63/63 [==============================] - 58s 916ms/step - loss: 0.4929 - accuracy: 0.8680 - val_loss: 1.0068 - val_accuracy: 0.8013\n",
      "Epoch 84/180\n",
      "63/63 [==============================] - 56s 887ms/step - loss: 0.4854 - accuracy: 0.8691 - val_loss: 1.0076 - val_accuracy: 0.7998\n",
      "Epoch 85/180\n",
      "63/63 [==============================] - 57s 907ms/step - loss: 0.4789 - accuracy: 0.8703 - val_loss: 0.9983 - val_accuracy: 0.8041\n",
      "Epoch 86/180\n",
      "63/63 [==============================] - 56s 895ms/step - loss: 0.4721 - accuracy: 0.8718 - val_loss: 0.9996 - val_accuracy: 0.8027\n",
      "Epoch 87/180\n",
      "63/63 [==============================] - 58s 918ms/step - loss: 0.4657 - accuracy: 0.8724 - val_loss: 0.9985 - val_accuracy: 0.8032\n",
      "Epoch 88/180\n",
      "63/63 [==============================] - 56s 897ms/step - loss: 0.4596 - accuracy: 0.8737 - val_loss: 1.0010 - val_accuracy: 0.8050\n",
      "Epoch 89/180\n",
      "63/63 [==============================] - 57s 899ms/step - loss: 0.4538 - accuracy: 0.8748 - val_loss: 0.9943 - val_accuracy: 0.8046\n",
      "Epoch 90/180\n",
      "63/63 [==============================] - 56s 885ms/step - loss: 0.4480 - accuracy: 0.8754 - val_loss: 0.9930 - val_accuracy: 0.8023\n",
      "Epoch 91/180\n",
      "63/63 [==============================] - 57s 909ms/step - loss: 0.4429 - accuracy: 0.8765 - val_loss: 0.9900 - val_accuracy: 0.8030\n",
      "Epoch 92/180\n",
      "63/63 [==============================] - 56s 894ms/step - loss: 0.4369 - accuracy: 0.8777 - val_loss: 0.9893 - val_accuracy: 0.8032\n",
      "Epoch 93/180\n",
      "63/63 [==============================] - 56s 897ms/step - loss: 0.4314 - accuracy: 0.8786 - val_loss: 0.9912 - val_accuracy: 0.8041\n",
      "Epoch 94/180\n",
      "63/63 [==============================] - 57s 901ms/step - loss: 0.4263 - accuracy: 0.8795 - val_loss: 0.9921 - val_accuracy: 0.8056\n",
      "Epoch 95/180\n",
      "63/63 [==============================] - 57s 908ms/step - loss: 0.4209 - accuracy: 0.8799 - val_loss: 0.9862 - val_accuracy: 0.8043\n",
      "Epoch 96/180\n",
      "63/63 [==============================] - 57s 905ms/step - loss: 0.4152 - accuracy: 0.8812 - val_loss: 0.9874 - val_accuracy: 0.8061\n",
      "Epoch 97/180\n",
      "63/63 [==============================] - 57s 908ms/step - loss: 0.4098 - accuracy: 0.8822 - val_loss: 0.9822 - val_accuracy: 0.8051\n",
      "Epoch 98/180\n",
      "63/63 [==============================] - 56s 895ms/step - loss: 0.4049 - accuracy: 0.8828 - val_loss: 0.9813 - val_accuracy: 0.8053\n",
      "Epoch 99/180\n",
      "63/63 [==============================] - 58s 925ms/step - loss: 0.3994 - accuracy: 0.8839 - val_loss: 0.9895 - val_accuracy: 0.8015\n",
      "Epoch 100/180\n",
      "63/63 [==============================] - 57s 913ms/step - loss: 0.3952 - accuracy: 0.8846 - val_loss: 0.9804 - val_accuracy: 0.8058\n",
      "Epoch 101/180\n",
      "63/63 [==============================] - 56s 898ms/step - loss: 0.3907 - accuracy: 0.8856 - val_loss: 0.9826 - val_accuracy: 0.8063\n",
      "Epoch 102/180\n",
      "63/63 [==============================] - 57s 911ms/step - loss: 0.3858 - accuracy: 0.8868 - val_loss: 0.9807 - val_accuracy: 0.8071\n",
      "Epoch 103/180\n",
      "63/63 [==============================] - 57s 903ms/step - loss: 0.3817 - accuracy: 0.8871 - val_loss: 0.9871 - val_accuracy: 0.8065\n",
      "Epoch 104/180\n",
      "63/63 [==============================] - 57s 912ms/step - loss: 0.3777 - accuracy: 0.8878 - val_loss: 0.9881 - val_accuracy: 0.8069\n",
      "Epoch 105/180\n",
      "63/63 [==============================] - 57s 901ms/step - loss: 0.3744 - accuracy: 0.8888 - val_loss: 0.9798 - val_accuracy: 0.8082\n",
      "Epoch 106/180\n",
      "63/63 [==============================] - 57s 912ms/step - loss: 0.3700 - accuracy: 0.8892 - val_loss: 0.9800 - val_accuracy: 0.8070\n",
      "Epoch 107/180\n",
      "63/63 [==============================] - 57s 906ms/step - loss: 0.3661 - accuracy: 0.8899 - val_loss: 0.9842 - val_accuracy: 0.8077\n",
      "Epoch 108/180\n",
      "63/63 [==============================] - 57s 911ms/step - loss: 0.3624 - accuracy: 0.8905 - val_loss: 0.9851 - val_accuracy: 0.8073\n",
      "Epoch 109/180\n",
      "63/63 [==============================] - 57s 909ms/step - loss: 0.3591 - accuracy: 0.8913 - val_loss: 0.9805 - val_accuracy: 0.8070\n",
      "Epoch 110/180\n",
      "63/63 [==============================] - 57s 909ms/step - loss: 0.3557 - accuracy: 0.8916 - val_loss: 0.9809 - val_accuracy: 0.8074\n",
      "Epoch 111/180\n",
      "63/63 [==============================] - 58s 916ms/step - loss: 0.3511 - accuracy: 0.8933 - val_loss: 0.9847 - val_accuracy: 0.8031\n",
      "Epoch 112/180\n",
      "63/63 [==============================] - 57s 900ms/step - loss: 0.3481 - accuracy: 0.8929 - val_loss: 0.9878 - val_accuracy: 0.8090\n",
      "Epoch 113/180\n",
      "63/63 [==============================] - 57s 913ms/step - loss: 0.3447 - accuracy: 0.8935 - val_loss: 0.9837 - val_accuracy: 0.8071\n",
      "Epoch 114/180\n",
      "63/63 [==============================] - 56s 892ms/step - loss: 0.3417 - accuracy: 0.8942 - val_loss: 0.9794 - val_accuracy: 0.8063\n",
      "Epoch 115/180\n",
      "63/63 [==============================] - 57s 910ms/step - loss: 0.3379 - accuracy: 0.8950 - val_loss: 0.9814 - val_accuracy: 0.8069\n",
      "Epoch 116/180\n",
      "63/63 [==============================] - 56s 895ms/step - loss: 0.3351 - accuracy: 0.8955 - val_loss: 0.9815 - val_accuracy: 0.8058\n",
      "Epoch 117/180\n",
      "63/63 [==============================] - 57s 898ms/step - loss: 0.3321 - accuracy: 0.8959 - val_loss: 0.9811 - val_accuracy: 0.8058\n",
      "Epoch 118/180\n",
      "63/63 [==============================] - 57s 910ms/step - loss: 0.3291 - accuracy: 0.8964 - val_loss: 0.9811 - val_accuracy: 0.8061\n",
      "Epoch 119/180\n",
      "63/63 [==============================] - 56s 884ms/step - loss: 0.3252 - accuracy: 0.8973 - val_loss: 0.9827 - val_accuracy: 0.8058\n",
      "Epoch 120/180\n",
      "63/63 [==============================] - 56s 885ms/step - loss: 0.3230 - accuracy: 0.8975 - val_loss: 0.9825 - val_accuracy: 0.8053\n",
      "Epoch 121/180\n",
      "63/63 [==============================] - 56s 887ms/step - loss: 0.3198 - accuracy: 0.8982 - val_loss: 0.9827 - val_accuracy: 0.8079\n",
      "Epoch 122/180\n",
      "63/63 [==============================] - 56s 889ms/step - loss: 0.3170 - accuracy: 0.8986 - val_loss: 0.9855 - val_accuracy: 0.8061\n",
      "Epoch 123/180\n",
      "63/63 [==============================] - 58s 917ms/step - loss: 0.3140 - accuracy: 0.8993 - val_loss: 0.9861 - val_accuracy: 0.8086\n",
      "Epoch 124/180\n",
      "63/63 [==============================] - 57s 899ms/step - loss: 0.3114 - accuracy: 0.8997 - val_loss: 0.9867 - val_accuracy: 0.8071\n",
      "Epoch 125/180\n",
      "63/63 [==============================] - 58s 917ms/step - loss: 0.3084 - accuracy: 0.9006 - val_loss: 0.9842 - val_accuracy: 0.8071\n",
      "Epoch 126/180\n",
      "63/63 [==============================] - 57s 900ms/step - loss: 0.3059 - accuracy: 0.9011 - val_loss: 0.9860 - val_accuracy: 0.8080\n",
      "Epoch 127/180\n",
      "63/63 [==============================] - 57s 908ms/step - loss: 0.3028 - accuracy: 0.9012 - val_loss: 0.9861 - val_accuracy: 0.8073\n",
      "Epoch 128/180\n",
      "63/63 [==============================] - 58s 915ms/step - loss: 0.3013 - accuracy: 0.9016 - val_loss: 0.9871 - val_accuracy: 0.8068\n",
      "Epoch 129/180\n",
      "63/63 [==============================] - 58s 915ms/step - loss: 0.2978 - accuracy: 0.9018 - val_loss: 0.9898 - val_accuracy: 0.8074\n",
      "Epoch 130/180\n",
      "63/63 [==============================] - 59s 936ms/step - loss: 0.2956 - accuracy: 0.9028 - val_loss: 0.9923 - val_accuracy: 0.8045\n",
      "Epoch 131/180\n",
      "63/63 [==============================] - 61s 975ms/step - loss: 0.2933 - accuracy: 0.9033 - val_loss: 0.9936 - val_accuracy: 0.8088\n",
      "Epoch 132/180\n",
      "63/63 [==============================] - 60s 947ms/step - loss: 0.2907 - accuracy: 0.9034 - val_loss: 0.9951 - val_accuracy: 0.8051\n",
      "Epoch 133/180\n",
      "63/63 [==============================] - 59s 937ms/step - loss: 0.2887 - accuracy: 0.9034 - val_loss: 0.9952 - val_accuracy: 0.8057\n",
      "Epoch 134/180\n",
      "63/63 [==============================] - 60s 953ms/step - loss: 0.2864 - accuracy: 0.9040 - val_loss: 0.9986 - val_accuracy: 0.8039\n",
      "Epoch 135/180\n",
      "63/63 [==============================] - 62s 992ms/step - loss: 0.2839 - accuracy: 0.9044 - val_loss: 0.9921 - val_accuracy: 0.8067\n",
      "Epoch 136/180\n",
      "63/63 [==============================] - 58s 921ms/step - loss: 0.2819 - accuracy: 0.9049 - val_loss: 0.9953 - val_accuracy: 0.8065\n",
      "Epoch 137/180\n",
      "63/63 [==============================] - 58s 926ms/step - loss: 0.2796 - accuracy: 0.9052 - val_loss: 0.9934 - val_accuracy: 0.8071\n",
      "Epoch 138/180\n",
      "63/63 [==============================] - 58s 919ms/step - loss: 0.2782 - accuracy: 0.9051 - val_loss: 0.9981 - val_accuracy: 0.8073\n",
      "Epoch 139/180\n",
      "63/63 [==============================] - 57s 911ms/step - loss: 0.2755 - accuracy: 0.9058 - val_loss: 0.9995 - val_accuracy: 0.8045\n",
      "Epoch 140/180\n",
      "63/63 [==============================] - 59s 933ms/step - loss: 0.2731 - accuracy: 0.9063 - val_loss: 0.9997 - val_accuracy: 0.8067\n",
      "Epoch 141/180\n",
      "63/63 [==============================] - 58s 930ms/step - loss: 0.2715 - accuracy: 0.9062 - val_loss: 1.0046 - val_accuracy: 0.8047\n",
      "Epoch 142/180\n",
      "63/63 [==============================] - 59s 935ms/step - loss: 0.2692 - accuracy: 0.9074 - val_loss: 1.0046 - val_accuracy: 0.8041\n",
      "Epoch 143/180\n",
      "63/63 [==============================] - 57s 913ms/step - loss: 0.2673 - accuracy: 0.9076 - val_loss: 1.0035 - val_accuracy: 0.8076\n",
      "Epoch 144/180\n",
      "63/63 [==============================] - 58s 925ms/step - loss: 0.2652 - accuracy: 0.9075 - val_loss: 1.0098 - val_accuracy: 0.8086\n",
      "Epoch 145/180\n",
      "63/63 [==============================] - 58s 914ms/step - loss: 0.2635 - accuracy: 0.9083 - val_loss: 1.0053 - val_accuracy: 0.8057\n",
      "Epoch 146/180\n",
      "63/63 [==============================] - 57s 913ms/step - loss: 0.2618 - accuracy: 0.9083 - val_loss: 1.0091 - val_accuracy: 0.8088\n",
      "Epoch 147/180\n",
      "63/63 [==============================] - 58s 926ms/step - loss: 0.2597 - accuracy: 0.9084 - val_loss: 1.0094 - val_accuracy: 0.8051\n",
      "Epoch 148/180\n",
      "63/63 [==============================] - 58s 928ms/step - loss: 0.2581 - accuracy: 0.9084 - val_loss: 1.0124 - val_accuracy: 0.8075\n",
      "Epoch 149/180\n",
      "63/63 [==============================] - 56s 897ms/step - loss: 0.2563 - accuracy: 0.9090 - val_loss: 1.0154 - val_accuracy: 0.8039\n",
      "Epoch 150/180\n",
      "63/63 [==============================] - 58s 918ms/step - loss: 0.2547 - accuracy: 0.9092 - val_loss: 1.0149 - val_accuracy: 0.8080\n",
      "Epoch 151/180\n",
      "63/63 [==============================] - 57s 912ms/step - loss: 0.2529 - accuracy: 0.9096 - val_loss: 1.0108 - val_accuracy: 0.8057\n",
      "Epoch 152/180\n",
      "63/63 [==============================] - 58s 923ms/step - loss: 0.2511 - accuracy: 0.9102 - val_loss: 1.0125 - val_accuracy: 0.8062\n",
      "Epoch 153/180\n",
      "63/63 [==============================] - 58s 921ms/step - loss: 0.2498 - accuracy: 0.9101 - val_loss: 1.0139 - val_accuracy: 0.8076\n",
      "Epoch 154/180\n",
      "63/63 [==============================] - 58s 918ms/step - loss: 0.2481 - accuracy: 0.9105 - val_loss: 1.0164 - val_accuracy: 0.8060\n",
      "Epoch 155/180\n",
      "63/63 [==============================] - 58s 920ms/step - loss: 0.2462 - accuracy: 0.9108 - val_loss: 1.0163 - val_accuracy: 0.8068\n",
      "Epoch 156/180\n",
      "63/63 [==============================] - 58s 917ms/step - loss: 0.2451 - accuracy: 0.9107 - val_loss: 1.0191 - val_accuracy: 0.8054\n",
      "Epoch 157/180\n",
      "63/63 [==============================] - 59s 931ms/step - loss: 0.2438 - accuracy: 0.9112 - val_loss: 1.0189 - val_accuracy: 0.8057\n",
      "Epoch 158/180\n",
      "63/63 [==============================] - 71s 1s/step - loss: 0.2417 - accuracy: 0.9120 - val_loss: 1.0212 - val_accuracy: 0.8059\n",
      "Epoch 159/180\n",
      "63/63 [==============================] - 73s 1s/step - loss: 0.2402 - accuracy: 0.9121 - val_loss: 1.0206 - val_accuracy: 0.8068\n",
      "Epoch 160/180\n",
      "63/63 [==============================] - 73s 1s/step - loss: 0.2390 - accuracy: 0.9118 - val_loss: 1.0232 - val_accuracy: 0.8055\n",
      "Epoch 161/180\n",
      "63/63 [==============================] - 74s 1s/step - loss: 0.2379 - accuracy: 0.9122 - val_loss: 1.0259 - val_accuracy: 0.8051\n",
      "Epoch 162/180\n",
      "63/63 [==============================] - 73s 1s/step - loss: 0.2362 - accuracy: 0.9120 - val_loss: 1.0303 - val_accuracy: 0.8070\n",
      "Epoch 163/180\n",
      "63/63 [==============================] - 72s 1s/step - loss: 0.2346 - accuracy: 0.9127 - val_loss: 1.0284 - val_accuracy: 0.8064\n",
      "Epoch 164/180\n",
      "63/63 [==============================] - 60s 955ms/step - loss: 0.2336 - accuracy: 0.9132 - val_loss: 1.0269 - val_accuracy: 0.8058\n",
      "Epoch 165/180\n",
      "63/63 [==============================] - 58s 919ms/step - loss: 0.2324 - accuracy: 0.9129 - val_loss: 1.0287 - val_accuracy: 0.8062\n",
      "Epoch 166/180\n",
      "63/63 [==============================] - 58s 923ms/step - loss: 0.2312 - accuracy: 0.9133 - val_loss: 1.0298 - val_accuracy: 0.8084\n",
      "Epoch 167/180\n",
      "63/63 [==============================] - 57s 913ms/step - loss: 0.2296 - accuracy: 0.9136 - val_loss: 1.0365 - val_accuracy: 0.8086\n",
      "Epoch 168/180\n",
      "63/63 [==============================] - 59s 930ms/step - loss: 0.2286 - accuracy: 0.9137 - val_loss: 1.0324 - val_accuracy: 0.8072\n",
      "Epoch 169/180\n",
      "63/63 [==============================] - 60s 952ms/step - loss: 0.2275 - accuracy: 0.9135 - val_loss: 1.0332 - val_accuracy: 0.8066\n",
      "Epoch 170/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 61s 964ms/step - loss: 0.2259 - accuracy: 0.9141 - val_loss: 1.0349 - val_accuracy: 0.8063\n",
      "Epoch 171/180\n",
      "63/63 [==============================] - 66s 1s/step - loss: 0.2244 - accuracy: 0.9144 - val_loss: 1.0357 - val_accuracy: 0.8049\n",
      "Epoch 172/180\n",
      "63/63 [==============================] - 65s 1s/step - loss: 0.2238 - accuracy: 0.9142 - val_loss: 1.0348 - val_accuracy: 0.8063\n",
      "Epoch 173/180\n",
      "63/63 [==============================] - 67s 1s/step - loss: 0.2227 - accuracy: 0.9146 - val_loss: 1.0364 - val_accuracy: 0.8054\n",
      "Epoch 174/180\n",
      "63/63 [==============================] - 72s 1s/step - loss: 0.2212 - accuracy: 0.9147 - val_loss: 1.0422 - val_accuracy: 0.8091\n",
      "Epoch 175/180\n",
      "63/63 [==============================] - 71s 1s/step - loss: 0.2201 - accuracy: 0.9150 - val_loss: 1.0397 - val_accuracy: 0.8077\n",
      "Epoch 176/180\n",
      "63/63 [==============================] - 72s 1s/step - loss: 0.2195 - accuracy: 0.9148 - val_loss: 1.0459 - val_accuracy: 0.8085\n",
      "Epoch 177/180\n",
      "63/63 [==============================] - 71s 1s/step - loss: 0.2175 - accuracy: 0.9159 - val_loss: 1.0449 - val_accuracy: 0.8064\n",
      "Epoch 178/180\n",
      "63/63 [==============================] - 69s 1s/step - loss: 0.2168 - accuracy: 0.9159 - val_loss: 1.0456 - val_accuracy: 0.8047\n",
      "Epoch 179/180\n",
      "63/63 [==============================] - 72s 1s/step - loss: 0.2157 - accuracy: 0.9157 - val_loss: 1.0487 - val_accuracy: 0.8050\n",
      "Epoch 180/180\n",
      "63/63 [==============================] - 71s 1s/step - loss: 0.2148 - accuracy: 0.9159 - val_loss: 1.0514 - val_accuracy: 0.8081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1353a2990>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tmp_x, padded_ita_sentences_train, batch_size=512, epochs=180,validation_split=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d8bdf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 12s 38ms/step - loss: 0.9975 - accuracy: 0.8099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9975460171699524, 0.8099499940872192]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = pad_sequences(padded_eng_sentences_test, maxlen_ita)\n",
    "model.evaluate(test_x,padded_ita_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc14e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 10s 27ms/step\n",
      "tom is awake.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> tom è sveglio\n",
      "----------------------------------------------------------\n",
      "\n",
      "i just did it.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> io l'ho appena fatto\n",
      "----------------------------------------------------------\n",
      "\n",
      "tom is brilliant.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> tom è brillante\n",
      "----------------------------------------------------------\n",
      "\n",
      "allow me to help.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> deve aiutare\n",
      "----------------------------------------------------------\n",
      "\n",
      "we heard tom.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> sentito tom\n",
      "----------------------------------------------------------\n",
      "\n",
      "don't waste my time.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> si del i mio tempo\n",
      "----------------------------------------------------------\n",
      "\n",
      "you were sick.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> eri malata\n",
      "----------------------------------------------------------\n",
      "\n",
      "tom plays piano.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> tom a pianoforte\n",
      "----------------------------------------------------------\n",
      "\n",
      "i'm resting now.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> sto lavorato adesso\n",
      "----------------------------------------------------------\n",
      "\n",
      "just say yes.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> dite dite di sì\n",
      "----------------------------------------------------------\n",
      "\n",
      "ask my friends.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> chiedete alle mie amici\n",
      "----------------------------------------------------------\n",
      "\n",
      "you'll die in jail.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> morirete in prigione\n",
      "----------------------------------------------------------\n",
      "\n",
      "we speak french.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> saremo in francese\n",
      "----------------------------------------------------------\n",
      "\n",
      "how can you lose?\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> come può perdere\n",
      "----------------------------------------------------------\n",
      "\n",
      "please try one.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> provane uno per favore\n",
      "----------------------------------------------------------\n",
      "\n",
      "i know that.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> io lo so\n",
      "----------------------------------------------------------\n",
      "\n",
      "i know tom likes it.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> a tom lo piace\n",
      "----------------------------------------------------------\n",
      "\n",
      "give me an apple.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> dia una mela\n",
      "----------------------------------------------------------\n",
      "\n",
      "excuse the mess.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> scusa per il casino\n",
      "----------------------------------------------------------\n",
      "\n",
      "tom won't miss you.\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> tom non mi aiutando\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb_preds = model.predict(test_x)\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "for i in range(0,20):\n",
    "    print(X_test[i])\n",
    "    print(logits_to_text(emb_preds[i],ita_tokenizer))\n",
    "    print(\"----------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bdb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d9a0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

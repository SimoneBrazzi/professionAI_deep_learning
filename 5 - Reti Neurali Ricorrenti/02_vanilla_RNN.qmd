---
title: "Vanilla RNN"
author: "Simone Brazzi"
jupyter: "r-tf"
format:
  html:
    theme:
      dark: "darkly"
      light: "flatly"
execute: 
  warning: false
self_contained: true
toc: true
toc-depth: 2
number-sections: true
editor: 
  markdown: 
    wrap: sentence
editor_options: 
  chunk_output_type: console
---

# Sentiment Analysis
```{python}
import warnings
warnings.filterwarnings("ignore")
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import tensorflow as tf
# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
import pandas as pd
import numpy as np
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.backend import clear_session

tf.config.set_visible_devices([], 'GPU')

```

# IMBD dataset

[link](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
50k recensioni con una colonne di sentiment positivo o negativo.

```{python}
path = "/Users/simonebrazzi/datasets/IMDB Dataset.csv"
df = pd.read_csv(path)
df.head()
```

```{python}
df = df.sample(10000, random_state=1)
```

```{python}
x = df.review.values
y = df.sentiment.values

le = LabelEncoder()
y = le.fit_transform(y)

xtrain, xtest, ytrain, ytest = train_test_split(
  x, 
  y,
  test_size=.2,
  random_state=1
)
```

Instanzio un oggetto tokenizer, per costruire la sequenza in modo corretto a partire dal dato non strutturato (il testo).
```{python}
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
```

Costruisco il vocabolario
```{python}
tokenizer.fit_on_texts(xtrain)
```

Costruisco la sequenza per train e test basandomi sul vocabolario costruito con il tokenizer.
```{python}
train_seq = tokenizer.texts_to_sequences(xtrain)
test_seq = tokenizer.texts_to_sequences(xtest)
```

```{python}
vocabulary = len(tokenizer.word_index) + 1
vocabulary
```

Troviamo la max len per le sequenze in train per paddare.
```{python}
maxlen = len(max(train_seq, key=len))
maxlen
```
Con questo si evita il *vanishing gradient*.

Paddiamo le sequenze basandosi con la max len trovata.
```{python}
padded_train_sequences = pad_sequences(train_seq, maxlen=maxlen)
padded_test_sequences = pad_sequences(test_seq, maxlen=maxlen)
```

```{python}
padded_train_sequences.shape
```
8000 sequenze di train con max len di 1853, di cui molte paddate con diversi zeri per arrivare a 1853.

# Vanilla RNN

Il modello Ã¨ composto da 3 layer:
- Embedding layer, con size di ingresso la vocabulary, output 128 e con input shape di maxlen.
- SimpleRNN layer, quindi un layer Vanilla. Prende in input il numero di record e il numero di features.
- Dense layer, con 1 neurone e attivazione sigmoid, per la classificazione binaria.

```{python}
clear_session()
model_RNN = Sequential()
model_RNN.add(Embedding(
  input_dim=vocabulary,
  output_dim=128,
  input_shape=(maxlen,)
  ))
model_RNN.add(SimpleRNN(64))
model_RNN.add(Dense(1, activation="sigmoid"))
model_RNN.summary()
```

Come optimizer keras consiglia **rmsprop** per ridurre il vanishing gradient. Aiuta la RNN a non dimenticare le informazioni passate con una mobile avg del gradiente al quadrato, mano a mano che si avanza nelle iterazioni.

```{python}
model_RNN.compile(
  optimizer="rmsprop",
  loss="binary_crossentropy",
  metrics=["accuracy"]
)
```

```{python}
model_RNN.fit(
  padded_train_sequences,
  ytrain,
  epochs=5,
  batch_size=256,
  validation_split=.2
)
```

```{python}
model.evaluate(padded_test_sequences, ytest)
```
